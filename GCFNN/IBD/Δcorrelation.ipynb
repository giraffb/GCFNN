{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05fc8be8-8c1d-44ec-b98e-6699f461b2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, os\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from GCFNN import GCFNN\n",
    "import torch.nn.functional as F\n",
    "from numpy.random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f360533f-c929-4713-aacb-9bf93cfca743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenated_tensor(X_set, M):\n",
    "    concatenated_tensor_list = []\n",
    "    # 遍历索引范围为 0 到 M-1 的值\n",
    "    for i in range(M.shape[1]):\n",
    "        # 拼接 X_set[i]\n",
    "        concatenated_tensor_list.append(X_set[i])\n",
    "        ## 将 adj_list[i] 转换为密集张量并拼接\n",
    "        #adj_dense_tensor = adj_list[i].to_dense()\n",
    "        #concatenated_tensor_list.append(adj_dense_tensor)\n",
    "        \n",
    "    # 将 tr_M 转换为张量并拼接\n",
    "    M_tensor = torch.from_numpy(M)\n",
    "    concatenated_tensor_list.append(M_tensor)\n",
    "    # 沿着 dim=1 拼接所有张量\n",
    "    concatenated_tensor = torch.cat(concatenated_tensor_list, dim=1)\n",
    "    return concatenated_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4c77429-d83b-4b1c-bca7-250cca95c22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入各组学数据表： (1638, 578) (546, 596) (2891, 1)\n",
      "Index(['PSM7J1BF', 'CSM7KORK', 'HSM7J4HS', 'MSM9VZNL', 'MSMB4LYB', 'MSM6J2PM',\n",
      "       'MSMA26ET', 'CSM79HMN', 'CSM67UB1', 'HSM7J4IS',\n",
      "       ...\n",
      "       'PSM7J134', 'HSM5FZBJ', 'CSM79HK1', 'CSM5MCTZ', 'CSM9X21P', 'CSM5MCXF',\n",
      "       'CSM67U9P', 'CSM7KOPQ', 'HSMA33LV', 'MSM5LLFK'],\n",
      "      dtype='object', length=1796)\n",
      "数据完整率missing_rate : 0.6080178173719376\n",
      "集合所有样本重构数据表： (1796, 578) (1796, 596) (1796, 1)\n",
      "标签转换为one-hot编码： (1796, 2) 459.0\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "meta_data=pd.read_csv('meta.csv',index_col=0)\n",
    "metabolomics=pd.read_csv('metabolomics.csv',index_col=0).fillna(0)\n",
    "metagenomics=pd.read_csv('metagenomics.csv',index_col=0)\n",
    "data_class={'nonIBD':0,'CD':1,'UC':1}\n",
    "meta_data['diagnosis']=meta_data['diagnosis'].map(data_class)\n",
    "\n",
    "x1=metagenomics\n",
    "x2=metabolomics\n",
    "print('输入各组学数据表：',x1.shape,x2.shape,meta_data.shape)\n",
    "\n",
    "#重构数据表和标签表\n",
    "X1=pd.concat([x1,x2],axis=1).iloc[:,:x1.shape[1]]\n",
    "X2=pd.concat([x1,x2],axis=1).iloc[:,x1.shape[1]:x1.shape[1]+x2.shape[1]]\n",
    "index_new=X1.index\n",
    "print(index_new)\n",
    "missing_rate=(len(x1)+len(x2))/(len(index_new)*2)\n",
    "print('数据完整率missing_rate :',missing_rate)\n",
    "                                        \n",
    "meta_data=meta_data.reindex(index_new)\n",
    "X1=array(X1.reindex(index_new))\n",
    "X2=array(X2.reindex(index_new))\n",
    "print('集合所有样本重构数据表：',X1.shape,X2.shape,meta_data.shape)\n",
    "X_set=[X1,X2]\n",
    "M        = len(X_set)\n",
    "Mask     = np.ones([np.shape(X_set[0])[0], M])\n",
    "#生成表示组学缺失情况的掩码矩阵，将缺失组学部分填补为0\n",
    "for m_idx in range(M):\n",
    "    Mask[np.isnan(X_set[m_idx]).all(axis=1), m_idx] = 0\n",
    "    #X_set[m_idx][Mask[:, m_idx] == 0] = np.mean(X_set[m_idx][Mask[:, m_idx] == 1], axis=0)\n",
    "    X_set[m_idx][Mask[:, m_idx] == 0] = 0\n",
    "    \n",
    "#对宏基因组数据预处理\n",
    "def log(base,x):\n",
    "    return np.log(x)/np.log(base)\n",
    "X_set[0]=log(2,2*X_set[0]+0.00001)\n",
    "\n",
    "#print(Mask)\n",
    "#print(X_set)\n",
    "Y = meta_data['diagnosis'].tolist()\n",
    "Y = np.array(Y)\n",
    "Y_onehot = np.zeros((Y.shape[0], 2))\n",
    "Y_onehot[np.arange(Y.shape[0]), Y] = 1\n",
    "print('标签转换为one-hot编码：', Y_onehot.shape, np.sum(Y_onehot[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45c38090-c77d-47d9-92bc-0784d3a815ff",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1: 0.9760\n",
      "Test ACC: 0.9639\n",
      "Test AUC: 0.9858\n",
      "you are test train_data\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "#os.makedirs('figure', exist_ok=True)\n",
    "folder_path = 'model_shap'\n",
    "cv = StratifiedKFold(n_splits=5,shuffle = True,random_state = 50)\n",
    "\n",
    "new_mg_names = ['s' + col.split('|s', 1)[1] for col in metagenomics.columns]\n",
    "combined_columns = new_mg_names + metabolomics.columns.to_list()\n",
    "for m in range(M):\n",
    "    combined_columns.append('mask'+str(m))\n",
    "train_index, test_index = next(cv.split(X_set[0],np.argmax(Y_onehot,axis=1)))\n",
    "tr_X_set, te_X_set, va_X_set = {}, {}, {}\n",
    "for m in range(len(X_set)):\n",
    "    tr_X_set[m],tr_Y_onehot,tr_M = X_set[m][train_index],Y_onehot[train_index],Mask[train_index]\n",
    "    te_X_set[m],te_Y_onehot,te_M = X_set[m][test_index],Y_onehot[test_index],Mask[test_index]\n",
    "    #归一化\n",
    "    #print(te_Y_onehot)\n",
    "def Normalize(data):\n",
    "    \"\"\"\n",
    "    :param data:Input data\n",
    "    :return:normalized data\n",
    "    \"\"\"\n",
    "    mean = np.mean(data)\n",
    "    mx = np.max(data)\n",
    "    mn = np.min(data)\n",
    "    return mean, mx, mn\n",
    "\n",
    "for m in range(M):\n",
    "    mean, mx, mn = Normalize(tr_X_set[m])\n",
    "    tr_X_set[m] = (tr_X_set[m] - mean) / (mx - mn)\n",
    "    te_X_set[m] = (te_X_set[m] - mean) / (mx - mn)\n",
    "    \n",
    "for m in range(M):\n",
    "    tr_X_set[m] = torch.from_numpy(tr_X_set[m])\n",
    "    te_X_set[m] = torch.from_numpy(te_X_set[m])\n",
    "tr_Y_onehot = torch.from_numpy(tr_Y_onehot)\n",
    "te_Y_onehot = torch.from_numpy(te_Y_onehot)\n",
    "\n",
    "#tansform\n",
    "tr_input = concatenated_tensor(tr_X_set, tr_M)\n",
    "te_input = concatenated_tensor(te_X_set, te_M)   \n",
    "model_path = os.path.join(folder_path, 'model_'+str(0)+'.pth')\n",
    "model_test = torch.load(model_path)\n",
    "\n",
    "if str(model_test.device) == 'cuda':\n",
    "    tr_input = tr_input.cuda()\n",
    "    te_input = te_input.cuda()\n",
    "    tr_Y_onehot = tr_Y_onehot.cuda()\n",
    "    te_Y_onehot = te_Y_onehot.cuda()\n",
    "model_test.predict_test(te_input, te_Y_onehot)\n",
    "\n",
    "# select a set of background examples to take an expectation over\n",
    "#background = tr_input[np.random.choice(tr_input.shape[0], 100, replace=False)]\n",
    "\n",
    "\n",
    "# explain predictions of the model on four images\n",
    "e = shap.DeepExplainer(model_test, tr_input)\n",
    "# ...or pass tensors directly\n",
    "# e = shap.DeepExplainer((model.layers[0].input, model.layers[-1].output), background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05ca86aa-fc83-479a-91a4-45aabe3a9a39",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#shap_values = e.shap_values(te_input)\n",
    "#print(te_input.shape)\n",
    "#shap.summary_plot(shap_values[0][:,:578], te_input[:,:578], max_display = 15, show = False)\n",
    "#plt.show()\n",
    "#shap.summary_plot(shap_values[0][:,578:-2], te_input[:,578:-2], max_display = 15, show = False)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efe1146d-7d27-4de2-baf1-442f69521a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_input = torch.cat((tr_input, te_input), dim=0)\n",
    "merged_input_index = index_new[train_index].tolist()+index_new[test_index].tolist()\n",
    "condition = (merged_input[:, -2:] == 1).all(axis=1)\n",
    "co_input = merged_input[condition]\n",
    "co_input_index = [idx for idx, cond in zip(merged_input_index, condition) if cond]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "176f4f59-e345-4b1d-85cf-d70059bb74ab",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 6.91 GiB (GPU 0; 31.75 GiB total capacity; 22.09 GiB already allocated; 1.33 GiB free; 29.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9fb94779f100>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshap_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mco_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mco_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombined_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mco_input_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"co_shap_values.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/shap/explainers/_deep/__init__.py\u001b[0m in \u001b[0;36mshap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mwere\u001b[0m \u001b[0mchosen\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0;34m\"top\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \"\"\"\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mranked_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_rank_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_additivity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_additivity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/shap/explainers/_deep/deep_pytorch.py\u001b[0m in \u001b[0;36mshap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# run attribution computation graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0mfeature_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_output_ranks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0msample_phis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m                 \u001b[0;31m# assign the attributions to the right part of the output arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/shap/explainers/_deep/deep_pytorch.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, idx, inputs)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0mselected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GCFNN/GCFNN_new/IBD/GCFNN.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, test_data)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;31m#u_set[m] = self.feature_selective_layer[m])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mx_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mmu_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecific_encoder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0mz_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreparametrize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mmu_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoint_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GCFNN/GCFNN_new/IBD/GCFNN.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, adj)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mx_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mx_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgat3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;31m#x_3 += self.trans_x(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1118\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GCFNN/GCFNN_new/IBD/GCFNN.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp, adj)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m    \u001b[0;31m# N 图的节点数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         \u001b[0ma_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m         \u001b[0;31m# [N, N, 2*out_features]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 6.91 GiB (GPU 0; 31.75 GiB total capacity; 22.09 GiB already allocated; 1.33 GiB free; 29.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "shap_values = e.shap_values(co_input)\n",
    "print(co_input.shape)\n",
    "pd.DataFrame(shap_values[0],columns = combined_columns, index = co_input_index).to_csv(\"co_shap_values.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c165da09-4c5c-4479-b82c-f93031f9138b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#不运行上段代码，直接读取已有shap数据\n",
    "shap_values={}\n",
    "co_shap_values = pd.read_csv(\"co_shap_values.csv\", index_col=0)\n",
    "shap_values[0] = co_shap_values.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ffb13b-174d-4ed8-98bb-4b5081fdf44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mg_names = ['s' + col.split('|s', 1)[1] for col in metagenomics.columns]\n",
    "combined_columns = new_mg_names + metabolomics.columns.to_list()\n",
    "for m in range(M):\n",
    "    combined_columns.append('mask'+str(m))\n",
    "    \n",
    "shap.initjs()\n",
    "shap.summary_plot(shap_values[0][:,:578], pd.DataFrame(array(co_input.cpu()),columns = combined_columns).iloc[:,:578], max_display = 20, show = False)\n",
    "plt.savefig('keymg_20.svg')\n",
    "plt.show()\n",
    "shap.summary_plot(shap_values[0][:,578:-2], pd.DataFrame(array(co_input.cpu()),columns = combined_columns).iloc[:,578:-2], max_display = 20, show = False)\n",
    "plt.savefig('keymb_20.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860b4c4f-1985-4a99-8572-98113ab8934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算平均的绝对值\n",
    "mean_abs_shap_values = np.mean(np.abs(shap_values[0]), axis=0)\n",
    "\n",
    "# 获取特征排名\n",
    "feature_ranking = np.argsort(-mean_abs_shap_values)\n",
    "\n",
    "#按排名打印特征\n",
    "mg_fe_all = open('co_mg_important_fe_all.txt',mode='a')\n",
    "mb_fe_all = open('co_mb_important_fe_all.txt',mode='a')\n",
    "combined_columns = metagenomics.columns.to_list() + metabolomics.columns.to_list()\n",
    "combined_columns[:578] = [col.split('|s__', 1)[1] for col in combined_columns[:578]]\n",
    "for j in range(len(feature_ranking)):\n",
    "    if feature_ranking[j] < 578:\n",
    "        mg_fe_all.writelines(str(feature_ranking[j])+'\\t'+combined_columns[feature_ranking[j]]+'\\n')\n",
    "    elif feature_ranking[j] <578+596:\n",
    "        mb_fe_all.writelines(str(feature_ranking[j]-578)+'\\t'+combined_columns[feature_ranking[j]]+'\\n')\n",
    "mg_fe_all.close()\n",
    "mb_fe_all.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12442bd-7e52-4535-a639-69f3c140f64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#取shap排名前二十的菌和代谢物用shap值作斯皮尔曼相关性分析，画聚类热力图\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# 加载CSV文件\n",
    "df = pd.read_csv('co_shap_values.csv', index_col=0)\n",
    "\n",
    "# 选择\"label\"列值为-1的行\n",
    "#selected_rows = df[df['label'] == -1]\n",
    "selected_rows = df\n",
    "print(selected_rows.shape)\n",
    "\n",
    "# 提取特征列\n",
    "mg_selected_columns = [105,384,431,355,92,106,569,73,281,17,315,298,319,391,71,152,26,157,159,70]\n",
    "mb_selected_columns = [541,439,462,578,438,593,543,545,515,580,337,526,554,550,441,16,575,413,436,450]\n",
    "mb_selected_columns = [578+index for index in mb_selected_columns]\n",
    "mg_features = selected_rows.iloc[:, mg_selected_columns]\n",
    "mb_features = selected_rows.iloc[:, mb_selected_columns]\n",
    "\n",
    "# 获取新列名的列表\n",
    "new_mg_names = ['|s' + col.split('|s', 1)[1] for col in mg_features.columns]\n",
    "\n",
    "# 重命名列名\n",
    "mg_features.columns = new_mg_names\n",
    "\n",
    "# 初始化相关性系数矩阵\n",
    "num_mg_features = len(mg_features.columns)\n",
    "num_mb_features = len(mb_features.columns)\n",
    "correlation_matrix_spearman = np.zeros((num_mg_features, num_mb_features))\n",
    "p_value_matrix_spearman = np.zeros((num_mg_features, num_mb_features))\n",
    "\n",
    "# 计算不同相关性系数\n",
    "for i, feature1 in enumerate(mg_features.columns):\n",
    "    for j, feature2 in enumerate(mb_features.columns):\n",
    "        correlation_spearman, p_value_spearman = stats.spearmanr(mg_features[feature1], mb_features[feature2])\n",
    "        correlation_matrix_spearman[i, j] = correlation_spearman\n",
    "        p_value_matrix_spearman[i, j] = p_value_spearman\n",
    "        \n",
    "# 设置p值阈值，例如，p值小于0.05\n",
    "p_value_threshold = 0.05\n",
    "\n",
    "# 根据p值修改热力图颜色块\n",
    "correlation_matrix_modified = np.where(p_value_matrix_spearman < p_value_threshold, correlation_matrix_spearman, np.nan)\n",
    "\n",
    "#增加shap正负性\n",
    "mb_shap = np.array([1,-1,-1,-1,1,-1,-1,-1,1,1,1,-1,-1,1,-1,-1,1,1,-1,1]) \n",
    "correlation_matrix_modified = np.vstack([correlation_matrix_modified, mb_shap])\n",
    "mg_shap = np.array([1,1,1,1,1,-1,-1,-1,-1,1,1,1,-1,1,1,1,-1,1,-1,1,np.nan]) \n",
    "mg_shap = mg_shap.reshape(-1, 1)  # 将一维数组转换为二维列向量\n",
    "correlation_matrix_modified = np.hstack([correlation_matrix_modified, mg_shap])\n",
    "\n",
    "# Create a heatmap with modified colors based on p-value\n",
    "mg_label = mg_features.columns.tolist()\n",
    "mg_label.append('SHAP_to_Healthy')\n",
    "mb_label = mb_features.columns.tolist()\n",
    "mb_label.append('SHAP_to_Healthy')\n",
    "\n",
    "plt.figure(figsize=(18, 12))\n",
    "sns.heatmap(correlation_matrix_modified, annot=True, fmt=\".2f\", cmap='coolwarm', xticklabels=mb_label, yticklabels=mg_label,vmin=-1,vmax=1)\n",
    "plt.title('Modified Spearman Correlation Heatmap based on p-value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b5e1e8-64b9-4653-908e-fad9de392763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将NaN值替换为一个正数（例如0）\n",
    "correlation_matrix_modified[np.isnan(correlation_matrix_modified)] = 0\n",
    "\n",
    "# 绘制聚类图\n",
    "plt.figure(figsize=(18, 12))\n",
    "sns.clustermap(correlation_matrix_modified, cmap='coolwarm', xticklabels=mb_label, yticklabels=mg_label,vmin=-1,vmax=1)\n",
    "plt.title('Clustered Spearman Correlation Heatmap based on p-value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7b248e-fcf3-4731-bb2c-5d63f7d7fd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#斯皮尔曼相关性\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "# 提取shap_values中的相关列\n",
    "selected_columns = [105,384,431,355,92,106,569,73,281,17,315,298,319,391,71,152,26,157,159,70]\n",
    "selected_values = shap_values[0][:, selected_columns]\n",
    "\n",
    "# 提取第579列到倒数第三列的数据\n",
    "reference_values = shap_values[0][:, 578:-2]\n",
    "# 创建一个DataFrame来存储相关性结果\n",
    "correlation_df = pd.DataFrame()\n",
    "\n",
    "# 计算每一列与参考列的斯皮尔曼相关性\n",
    "for i in range(selected_values.shape[1]):\n",
    "    selected_col = selected_values[:, i]\n",
    "    correlations = [spearmanr(selected_col, reference_col).correlation for reference_col in reference_values.T]\n",
    "    correlation_df[f\"Column {selected_columns[i]}\"] = correlations\n",
    "    \n",
    "# 将相关性表保存为CSV文件\n",
    "correlation_df = correlation_df.T\n",
    "new_mg_names = [col.split('|s__', 1)[1] for col in metagenomics.columns]\n",
    "correlation_df.index = [new_mg_names[i] for i in selected_columns]\n",
    "correlation_df.columns = metabolomics.columns\n",
    "correlation_df.to_csv(\"spearmanr_correlation_table.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc353e58-2135-4f49-b02b-3733bd498306",
   "metadata": {},
   "outputs": [],
   "source": [
    "##皮尔森相关性\n",
    "#\n",
    "#from scipy.stats import pearsonr \n",
    "## 提取shap_values中的相关列\n",
    "#selected_columns = [105,384,431,355,92,106,569,73,281,17]\n",
    "#selected_values = shap_values[0][:, selected_columns]\n",
    "#\n",
    "## 提取第579列到倒数第四列的数据（第579列到倒数第三列是n-4到n-1）\n",
    "#reference_values = shap_values[0][:, 578:-2]\n",
    "##pd.DataFrame(shap_values[0]).to_csv(\"co_shap_values.csv\", index=True)\n",
    "## 创建一个DataFrame来存储相关性结果\n",
    "#correlation_df = pd.DataFrame()\n",
    "#\n",
    "## 计算每一列与参考列的斯皮尔曼相关性\n",
    "#for i in range(selected_values.shape[1]):\n",
    "#    selected_col = selected_values[:, i]\n",
    "#    correlations = [pearsonr(selected_col, reference_col)[0] for reference_col in reference_values.T]\n",
    "#    correlation_df[f\"Column {selected_columns[i]}\"] = correlations\n",
    "#    \n",
    "## 将相关性表保存为CSV文件\n",
    "#correlation_df = correlation_df.T\n",
    "#new_mg_names = [col.split('|s__', 1)[1] for col in metagenomics.columns]\n",
    "#correlation_df.index = [new_mg_names[i] for i in selected_columns]\n",
    "#correlation_df.columns = metabolomics.columns\n",
    "#correlation_df.to_csv(\"pearsonr_correlation_table.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dea79b-d1b0-4eff-8159-e1e15d1b58b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 假设correlation_df是一个10*578的DataFrame，行名为父节点的名称\n",
    "# 假设每个行名与在该行中绝对值排名前二十的列名相连接\n",
    "# 假设您已经选择了前二十个相关性最高的列\n",
    "\n",
    "# 创建一个有向图\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# 添加根节点'IBD_to_normal'\n",
    "G.add_node('IBD_to_normal')\n",
    "\n",
    "# 添加父节点（行名），并连接到根节点\n",
    "for parent_node in correlation_df.index:\n",
    "    G.add_node(parent_node)\n",
    "    for correlation_value in [1,1,1,1,1,-1,-1,-1,-1,1,1,1,-1,1,1,1,-1,1,-1,1]:\n",
    "        G.add_edge('IBD_to_normal', parent_node, weight = correlation_value)\n",
    "\n",
    "# 添加子节点（前二十的列名）并连接到父节点，权重为相关性值\n",
    "for parent_node in correlation_df.index:\n",
    "    top_20_correlations = correlation_df.loc[parent_node].abs().nlargest(20)\n",
    "    top_20_correlations = correlation_df.loc[parent_node,top_20_correlations.index]\n",
    "    for child_node, correlation_value in top_20_correlations.iteritems():\n",
    "        G.add_node(child_node)\n",
    "        G.add_edge(parent_node, child_node, weight=correlation_value)\n",
    "\n",
    "# 调整节点位置以更好显示根节点\n",
    "pos = nx.spring_layout(G, seed=42, k=0.25, iterations=50)  # 调整k和iterations以更好显示根节点\n",
    "\n",
    "# 获取边的权重\n",
    "weights = [(u, v, f\"{G[u][v].get('weight', 1.0):.2f}\") for u, v in G.edges()]  # 保留小数点后两位并保留正负符号\n",
    "\n",
    "# 绘制网络图，包括权值标签\n",
    "plt.figure(figsize=(12, 12))\n",
    "nx.draw_networkx(G, pos, labels=None, node_size=1000, node_color='lightblue', font_size=10, font_color='black',\n",
    "                 edge_color='gray', width=1.0, alpha=0.7, with_labels=True, font_weight='bold', arrows=True)\n",
    "\n",
    "# 添加权值标签\n",
    "#edge_labels = {(u, v): label for u, v, label in weights}\n",
    "#nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)\n",
    "\n",
    "# 添加节点标签\n",
    "node_labels = {node: node for node in G.nodes()}\n",
    "nx.draw_networkx_labels(G, pos, labels=node_labels, font_size=10, font_color='black')\n",
    "\n",
    "plt.title(\"Network Graph of Correlations\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bc83ba-2286-464c-bfbf-4a95c33121c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "key = 0.5\n",
    "\n",
    "# 创建 'unweight.glasso_edge' 文件\n",
    "edge_file = 'unweight.glasso_edge' + str(key) + '.tsv'\n",
    "node_weight_mapping = {}\n",
    "with open(edge_file, 'w') as f:\n",
    "    # 写入具有新列名的表头\n",
    "    f.write(\"id\\tsource\\ttarget\\n\")\n",
    "\n",
    "    # 初始化链接计数器\n",
    "    link_counter = 1\n",
    "\n",
    "    # 初始化用于存储已写入的节点的集合\n",
    "    written_nodes = set()\n",
    "\n",
    "    # 写入相关性大于key的边，并逐渐增加链接名称\n",
    "    for idx in correlation_df.index:\n",
    "        written_nodes.add(idx)\n",
    "        node_weight_mapping[idx] = 100\n",
    "        for col in correlation_df.columns:\n",
    "            correlation_value = correlation_df.at[idx, col]\n",
    "            if correlation_value >= key:\n",
    "                print(idx,col)\n",
    "                # 写入边的信息\n",
    "                f.write(\"link_{}\\t{}\\t{}\\n\".format(link_counter, idx, col))\n",
    "                link_counter += 1\n",
    "\n",
    "                # 将节点添加到已写入的节点集合中\n",
    "                written_nodes.add(col)\n",
    "\n",
    "                # 创建节点到权值的映射\n",
    "                node_weight_mapping[col] = 10\n",
    "\n",
    "# 创建 'unweight.glasso_node' 文件\n",
    "node_file = 'unweight.glasso_node' + str(key) + '.tsv'\n",
    "with open(node_file, 'w') as f:\n",
    "    # 写入具有新列名的表头\n",
    "    f.write(\"id\\tlabel\\n\")\n",
    "\n",
    "    # 写入已写入的节点以及根据映射分配的权值\n",
    "    for node in written_nodes:\n",
    "        weight = node_weight_mapping.get(node, 0)  # 获取权值，如果未在映射中找到则默认为0\n",
    "        f.write(f\"{node}\\t{weight}\\n\")\n",
    "\n",
    "    # 添加 'IBD_to_normal' 节点，权值为1000\n",
    "    f.write(\"IBD_to_normal\\t200\\n\")\n",
    "\n",
    "# 更新 'unweight.glasso_edge' 文件以添加到 'IBD_to_normal' 的连接\n",
    "with open(edge_file, 'a') as f:\n",
    "    for idx in correlation_df.index:\n",
    "        # 写入到 'IBD_to_normal' 的连接\n",
    "        f.write(\"link_{}\\t{}\\tIBD_to_normal\\n\".format(link_counter, idx))\n",
    "        link_counter += 1\n",
    "\n",
    "# 创建一个ExcelWriter对象\n",
    "with pd.ExcelWriter('combined_file'+str(key)+'.xlsx', engine='xlsxwriter') as writer:\n",
    "    # 写入第一个TSV文件到第一个sheet\n",
    "    df1 = pd.read_csv('unweight.glasso_node'+str(key)+'.tsv', sep='\\t')\n",
    "    df1.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "\n",
    "    # 写入第二个TSV文件到第二个sheet\n",
    "    df2 = pd.read_csv('unweight.glasso_edge'+str(key)+'.tsv', sep='\\t')\n",
    "    df2.to_excel(writer, sheet_name='Sheet2', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30142d4-ae6a-4516-b29f-830df0264dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
